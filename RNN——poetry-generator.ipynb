{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据路径\n",
    "DATA_PATH = 'poetry.txt'\n",
    "# 单行诗最大长度\n",
    "MAX_LEN = 64\n",
    "# 禁用的字符，拥有以下符号的诗将被忽略\n",
    "DISALLOWED_WORDS = ['（', '）', '(', ')', '__', '《', '》', '【', '】', '[', ']']\n",
    "\n",
    "# 一首诗（一行）对应一个列表的元素\n",
    "poetry = []\n",
    "\n",
    "# 按行读取数据 poetry.txt\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "# 遍历处理每一条数据    \n",
    "for line in lines:\n",
    "    # 利用正则表达式拆分标题和内容\n",
    "    fields = re.split(r\"[:：]\", line)\n",
    "    # 跳过异常数据\n",
    "    if len(fields) != 2:\n",
    "        continue\n",
    "    # 得到诗词内容（后面不需要标题）\n",
    "    content = fields[1]\n",
    "    # 跳过内容过长的诗词\n",
    "    if len(content) > MAX_LEN - 2:\n",
    "        continue\n",
    "    # 跳过存在禁用符的诗词\n",
    "    if any(word in content for word in DISALLOWED_WORDS):\n",
    "        continue\n",
    "\n",
    "    poetry.append(content.replace('\\n', '')) # 最后要记得删除换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变，春逐鸟声开。初风飘带柳，晚雪间花梅。碧林青旧竹，绿沼翠新苔。芝田初雁去，绮树巧莺来。\n",
      "晚霞聊自怡，初晴弥可喜。日晃百花色，风动千林翠。池鱼跃不同，园鸟声还异。寄言博通者，知予物外志。\n",
      "夏律昨留灰，秋箭今移晷。峨嵋岫初出，洞庭波渐起。桂白发幽岩，菊黄开灞涘。运流方可叹，含毫属微理。\n",
      "寒惊蓟门叶，秋发小山枝。松阴背日转，竹影避风移。提壶菊花岸，高兴芙蓉池。欲知凉气早，巢空燕不窥。\n",
      "山亭秋色满，岩牖凉风度。疏兰尚染烟，残菊犹承露。古石衣新苔，新巢封古树。历览情无极，咫尺轮光暮。\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    print(poetry[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最小词频\n",
    "MIN_WORD_FREQUENCY = 8\n",
    "\n",
    "# 统计词频，利用Counter可以直接按单个字符进行统计词频\n",
    "counter = Counter()\n",
    "for line in poetry:\n",
    "    counter.update(line)\n",
    "# 过滤掉低词频的词\n",
    "tokens = [token for token, count in counter.items() if count >= MIN_WORD_FREQUENCY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒 -> 402\n",
      "随 -> 186\n",
      "穷 -> 75\n",
      "律 -> 46\n",
      "变 -> 85\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for token, count in counter.items():\n",
    "    if i >= 5:\n",
    "        break;\n",
    "    print(token, \"->\",count)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 补上特殊词标记：填充字符标记、未知词标记、开始标记、结束标记\n",
    "tokens = [\"[PAD]\", \"[NONE]\", \"[START]\", \"[END]\"] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 映射: 词 -> 编号\n",
    "word_idx = {}\n",
    "# 映射: 编号 -> 词\n",
    "idx_word = {}\n",
    "for idx, word in enumerate(tokens):\n",
    "    word_idx[word] = idx\n",
    "    idx_word[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    分词器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        # 词汇表大小\n",
    "        self.dict_size = len(tokens)\n",
    "        # 生成映射关系\n",
    "        self.token_id = {} # 映射: 词 -> 编号\n",
    "        self.id_token = {} # 映射: 编号 -> 词\n",
    "        for idx, word in enumerate(tokens):\n",
    "            self.token_id[word] = idx\n",
    "            self.id_token[idx] = word\n",
    "\n",
    "        # 各个特殊标记的编号id，方便其他地方使用\n",
    "        self.start_id = self.token_id[\"[START]\"]\n",
    "        self.end_id = self.token_id[\"[END]\"]\n",
    "        self.none_id = self.token_id[\"[NONE]\"]\n",
    "        self.pad_id = self.token_id[\"[PAD]\"]\n",
    "\n",
    "    def id_to_token(self, token_id):\n",
    "        \"\"\"\n",
    "        编号 -> 词\n",
    "        \"\"\"\n",
    "        return self.id_token.get(token_id)\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"\n",
    "        词 -> 编号\n",
    "        \"\"\"\n",
    "        return self.token_id.get(token, self.none_id)\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        \"\"\"\n",
    "        词列表 -> [START]编号 + 编号列表 + [END]编号\n",
    "        \"\"\"\n",
    "        token_ids = [self.start_id, ] # 起始标记\n",
    "        # 遍历，词转编号\n",
    "        for token in tokens:\n",
    "            token_ids.append(self.token_to_id(token))\n",
    "        token_ids.append(self.end_id) # 结束标记\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        编号列表 -> 词列表(去掉起始、结束标记)\n",
    "        \"\"\"\n",
    "        # 起始、结束标记\n",
    "        flag_tokens = {\"[START]\", \"[END]\"}\n",
    "\n",
    "        tokens = []\n",
    "        for idx in token_ids:\n",
    "            token = self.id_to_token(idx)\n",
    "            # 跳过起始、结束标记\n",
    "            if token not in flag_tokens:\n",
    "                tokens.append(token)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryDataSet:\n",
    "    \"\"\"\n",
    "    古诗数据集生成器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer, batch_size):\n",
    "        # 数据集\n",
    "        self.data = data\n",
    "        self.total_size = len(self.data)\n",
    "        # 分词器，用于词转编号\n",
    "        self.tokenizer = tokenizer\n",
    "        # 每批数据量\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        # 每个epoch迭代的步数\n",
    "        self.steps = int(math.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "    def pad_line(self, line, length, padding=None):\n",
    "        \"\"\"\n",
    "        对齐单行数据\n",
    "        \"\"\"\n",
    "        if padding is None:\n",
    "            padding = self.tokenizer.pad_id\n",
    "\n",
    "        padding_length = length - len(line)\n",
    "        if padding_length > 0:\n",
    "            return line + [padding] * padding_length\n",
    "        else:\n",
    "            return line[:length]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 打乱数据\n",
    "        np.random.shuffle(self.data)\n",
    "        # 迭代一个epoch，每次yield一个batch\n",
    "        for start in range(0, self.total_size, self.batch_size):\n",
    "            end = min(start + self.batch_size, self.total_size)\n",
    "            data = self.data[start:end]\n",
    "\n",
    "            max_length = max(map(len, data)) \n",
    "\n",
    "            batch_data = []\n",
    "            for str_line in data:\n",
    "                # 对每一行诗词进行编码、并补齐padding\n",
    "                encode_line = self.tokenizer.encode(str_line)\n",
    "                pad_encode_line = self.pad_line(encode_line, max_length + 2) # 加2是因为tokenizer.encode会添加START和END\n",
    "                batch_data.append(pad_encode_line)\n",
    "\n",
    "            batch_data = np.array(batch_data)\n",
    "            # yield 特征、标签\n",
    "            yield batch_data[:, :-1], batch_data[:, 1:]\n",
    "\n",
    "    def generator(self):\n",
    "        while True:\n",
    "            yield from self.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "#未设置BATCH_SIZE的值！！！\n",
    "BATCH_SIZE = 10\n",
    "##############\n",
    "dataset = PoetryDataSet(poetry, tokenizer, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0325 19:00:45.692135  8488 deprecation.py:506] From C:\\Users\\17408\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0325 19:00:45.697127  8488 deprecation.py:506] From C:\\Users\\17408\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # 词嵌入层\n",
    "    tf.keras.layers.Embedding(input_dim=tokenizer.dict_size, output_dim=150),\n",
    "    # 第一个LSTM层\n",
    "    tf.keras.layers.LSTM(150, dropout=0.5, return_sequences=True),\n",
    "    # 第二个LSTM层\n",
    "    tf.keras.layers.LSTM(150, dropout=0.5, return_sequences=True),\n",
    "    # 利用TimeDistributed对每个时间步的输出都做Dense操作(softmax激活)\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tokenizer.dict_size, activation='softmax')),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 150)         306000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 150)         180600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 150)         180600    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 2040)        308040    \n",
      "=================================================================\n",
      "Total params: 975,240\n",
      "Trainable params: 975,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(), \n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0325 19:00:46.419542  8488 deprecation.py:323] From C:\\Users\\17408\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 185s 448ms/step - loss: 5.4931\n",
      "Epoch 2/10\n",
      "163/413 [==========>...................] - ETA: 1:50 - loss: 5.0239"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    dataset.generator(), \n",
    "    steps_per_epoch=dataset.steps, \n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要先将词转为编号\n",
    "token_ids = [tokenizer.token_to_id(word) for word in [\"月\", \"光\", \"静\", \"谧\"]]\n",
    "# 进行预测\n",
    "result = model.predict([token_ids ,])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, token_ids):\n",
    "    \"\"\"\n",
    "    在概率值为前100的词中选取一个词(按概率分布的方式)\n",
    "    :return: 一个词的编号(不包含[PAD][NONE][START])\n",
    "    \"\"\"\n",
    "    # 预测各个词的概率分布\n",
    "    # -1 表示只要对最新的词的预测\n",
    "    # 3: 表示不要前面几个标记符\n",
    "    _probas = model.predict([token_ids, ])[0, -1, 3:]\n",
    "    # 按概率降序，取前100\n",
    "    p_args = _probas.argsort()[-100:][::-1] # 此时拿到的是索引\n",
    "    p = _probas[p_args] # 根据索引找到具体的概率值\n",
    "    p = p / sum(p) # 归一\n",
    "    # 按概率抽取一个\n",
    "    target_index = np.random.choice(len(p), p=p)\n",
    "    # 前面预测时删除了前几个标记符，因此编号要补上3位，才是实际在tokenizer词典中的编号\n",
    "    return p_args[target_index] + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.encode(\"清风明月\")[:-1]\n",
    "while len(token_ids) < 13:\n",
    "    # 预测词的编号\n",
    "    target = predict(model, token_ids)\n",
    "    # 保存结果\n",
    "    token_ids.append(target)\n",
    "    # 到达END\n",
    "    if target == tokenizer.end_id: \n",
    "        break\n",
    "\n",
    "print(\"\".join(tokenizer.decode(token_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_poem(tokenizer, model, text=\"\"):\n",
    "    \"\"\"\n",
    "    随机生成一首诗\n",
    "    :param tokenizer: 分词器\n",
    "    :param model: 古诗模型\n",
    "    :param text: 古诗的起始字符串，默认为空\n",
    "    :return: 一首古诗的字符串\n",
    "    \"\"\"\n",
    "    # 将初始字符串转成token_ids，并去掉结束标记[END]\n",
    "    token_ids = tokenizer.encode(text)[:-1]\n",
    "    while len(token_ids) < MAX_LEN:\n",
    "        # 预测词的编号\n",
    "        target = predict(model, token_ids)\n",
    "        # 保存结果\n",
    "        token_ids.append(target)\n",
    "        # 到达END\n",
    "        if target == tokenizer.end_id: \n",
    "            break\n",
    "\n",
    "    return \"\".join(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(generate_random_poem(tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_random_poem(tokenizer, model, \"春眠不觉晓，\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_acrostic_poem(tokenizer, model, heads):\n",
    "    \"\"\"\n",
    "    生成一首藏头诗\n",
    "    :param tokenizer: 分词器\n",
    "    :param model: 古诗模型\n",
    "    :param heads: 藏头诗的头\n",
    "    :return: 一首古诗的字符串\n",
    "    \"\"\"\n",
    "    # token_ids，只包含[START]编号\n",
    "    token_ids = [tokenizer.start_id, ]\n",
    "    # 逗号和句号标记编号\n",
    "    punctuation_ids = {tokenizer.token_to_id(\"，\"), tokenizer.token_to_id(\"。\")}\n",
    "    content = []\n",
    "    # 为每一个head生成一句诗\n",
    "    for head in heads:\n",
    "        content.append(head)\n",
    "        # head转为编号id，放入列表，用于预测\n",
    "        token_ids.append(tokenizer.token_to_id(head))\n",
    "        # 开始生成一句诗\n",
    "        target = -1;\n",
    "        while target not in punctuation_ids: # 遇到逗号、句号，说明本句结束，开始下一句\n",
    "            # 预测词的编号\n",
    "            target = predict(model, token_ids)\n",
    "            # 因为可能预测到END，所以加个判断\n",
    "            if target > 3:\n",
    "                # 保存结果到token_ids中，下一次预测还要用\n",
    "                token_ids.append(target)\n",
    "                content.append(tokenizer.id_to_token(target))\n",
    "\n",
    "    return \"\".join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_acrostic_poem(tokenizer, model, heads=\"上善若水\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
